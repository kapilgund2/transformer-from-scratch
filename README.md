# Transformer from Scratch

This repository contains an implementation of a Transformer model from scratch using **PyTorch**. The Transformer model, originally introduced in the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762), is widely used in Natural Language Processing (NLP) tasks such as machine translation, text generation, and language modeling.

## ðŸ“Œ Features
- Implements a **custom Transformer model** using PyTorch.
- Supports **multi-head self-attention**, **positional encoding**, and **feed-forward networks**.
- Implements **Bilingual Dataset handling** for training on parallel text corpora.
- Supports **tokenization and preprocessing** using Hugging Faceâ€™s Tokenizer.
- Provides **training scripts** with loss tracking and model saving.
- Supports **greedy decoding** for inference.
- Includes **model evaluation metrics** like BLEU, WER, and CER.

### Implementation of "Attention is all you need" paper
### English to German Translator
[Original Video](https://www.youtube.com/watch?v=ISNdQcPhsts&t=177s)<br>
[Credit](https://www.youtube.com/@umarjamilai/videos)
